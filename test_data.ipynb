{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request \n",
    "url = 'https://raw.githubusercontent.com/onecoinbuybus/Database_chemoinformatics/master/SRU_data.txt'\n",
    "urllib.request.urlretrieve(url, 'SRU_data.txt') \n",
    "\n",
    "sru_data = pd.read_csv('SRU_data.txt',header=None, skiprows=1).iloc[:,0].apply(lambda x: pd.Series(x.split()))\n",
    "sru_data.columns = ['MEA GAS',\n",
    "                    'AIR MEA1',\n",
    "                    'AIR MEA 2',\n",
    "                    'AIR SWS',\n",
    "                    'SWS GAS',\n",
    "                    'H2S',\n",
    "                    'SO2']\n",
    "sru_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(114514)\n",
    "total_X,total_Y = np.array(sru_data[['MEA GAS','AIR MEA1','AIR MEA 2','AIR SWS','SWS GAS']]),np.array(sru_data[['H2S','SO2']])\n",
    "length = int(0.7*total_X.shape[0])\n",
    "total_indexs = [i for i in range(total_X.shape[0])]\n",
    "train_indexs = random.sample(total_indexs,k=length)\n",
    "total_indexs = [i for i in range(total_X.shape[0])]\n",
    "val_test_indexs = []\n",
    "for idx in total_indexs:\n",
    "    if idx not in train_indexs:\n",
    "        val_test_indexs.append(idx)\n",
    "val_indexs = random.sample(val_test_indexs,k = int(0.67*len(val_test_indexs)))\n",
    "test_indexs = []\n",
    "for idx in val_test_indexs:\n",
    "    if idx not in val_indexs:\n",
    "        test_indexs.append(idx)\n",
    "# print(sorted(val_indexs)[0:10],sorted(test_indexs)[0:10])\n",
    "# print(sorted(train_indexs)[0:10])\n",
    "train_X,train_Y,val_X,val_Y,test_X,test_Y = total_X[train_indexs],total_Y[train_indexs],total_X[val_indexs],total_Y[val_indexs],total_X[test_indexs],total_Y[test_indexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用随机森林回归看看效果，发现效果也不错."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "best_val_score = 0.0\n",
    "best_md = 25\n",
    "best_ns = 100\n",
    "for md in range(1,32,2):\n",
    "    for ns in range(10,201,10):\n",
    "        regression_model = RandomForestRegressor(n_estimators=ns,max_depth = md)\n",
    "        regression_model.fit(train_X,train_Y)\n",
    "\n",
    "        print(\"Traing Score:%f\" % regression_model.score(train_X, train_Y))\n",
    "        val_score = regression_model.score(val_X, val_Y)\n",
    "        print(\"Testing Score:%f\" % val_score)\n",
    "        print('md:{},ns:{}'.format(md,ns))\n",
    "        print('         ')\n",
    "        if val_score >= best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_md = md\n",
    "            best_ns = ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_md,best_ns)\n",
    "regression_model = RandomForestRegressor(n_estimators=best_ns,max_depth = best_md)\n",
    "regression_model.fit(train_X,train_Y)\n",
    "print(\"Traing Score:%f\" % regression_model.score(train_X, train_Y))\n",
    "val_score = regression_model.score(val_X, val_Y)\n",
    "print(\"valing Score:%f\" % val_score)\n",
    "test_score = regression_model.score(test_X, test_Y)\n",
    "print(\"testing Score:%f\" % test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用transformer试试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "config = edict()\n",
    "config.hidden_size = 5\n",
    "config.intermediate_size = 5\n",
    "config.num_attention_heads = 1\n",
    "config.hidden_dropout_prob = 0.1\n",
    "config.attention_probs_dropout_prob = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class TrainablePositionalEncoding(nn.Module):\n",
    "#     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "#     def __init__(self, max_position_embeddings, hidden_size, dropout=0.1):\n",
    "#         super(TrainablePositionalEncoding, self).__init__()\n",
    "#         self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "#         self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, input_feat):\n",
    "#         #print('input_feat:{}'.format(input_feat.shape))\n",
    "#         bsz, seq_length = input_feat.shape[:2]\n",
    "#         position_ids = torch.arange(seq_length, dtype=torch.long, device=input_feat.device)\n",
    "#         position_ids = position_ids.unsqueeze(0).repeat(bsz, 1)  # (N, L)\n",
    "#         position_embeddings = self.position_embeddings(position_ids)\n",
    "#         embeddings = self.LayerNorm(input_feat + position_embeddings)\n",
    "#         embeddings = self.dropout(embeddings)\n",
    "#         return embeddings\n",
    "\n",
    "#     def add_position_emb(self, input_feat):\n",
    "#         bsz, seq_length = input_feat.shape[:2]\n",
    "#         position_ids = torch.arange(seq_length, dtype=torch.long, device=input_feat.device)\n",
    "#         position_ids = position_ids.unsqueeze(0).repeat(bsz, 1)  # (N, L)\n",
    "#         position_embeddings = self.position_embeddings(position_ids)\n",
    "#         return input_feat + position_embeddings\n",
    "\n",
    "\n",
    "# class LinearLayer(nn.Module):\n",
    "#     \"\"\"linear layer configurable with layer normalization, dropout, ReLU.\"\"\"\n",
    "#     def __init__(self, in_hsz, out_hsz, layer_norm=True, dropout=0.1, relu=True):\n",
    "#         super(LinearLayer, self).__init__()\n",
    "#         self.relu = relu\n",
    "#         self.layer_norm = layer_norm\n",
    "#         if layer_norm:\n",
    "#             self.LayerNorm = nn.LayerNorm(in_hsz)\n",
    "#         layers = [nn.Dropout(dropout), nn.Linear(in_hsz, out_hsz)]\n",
    "#         self.net = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"(N, L, D)\"\"\"\n",
    "#         if self.layer_norm:\n",
    "#             x = self.LayerNorm(x)\n",
    "#         x = self.net(x)\n",
    "#         if self.relu:\n",
    "#             x = F.relu(x, inplace=True)\n",
    "#         return x  # (N, L, D)\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\"The hidden size (%d) is not a multiple of the number of attention heads (%d)\" % (\n",
    "                config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, query_states, key_states, value_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_states: (N, D)\n",
    "            key_states: (N,D)\n",
    "            value_states: (N,D)\n",
    "            attention_mask: (N, Lq, L)\n",
    "        \"\"\"\n",
    "        # only need to mask the dimension where the softmax (last dim) is applied, as another dim (second last)\n",
    "        # will be ignored in future computation anyway\n",
    "        # attention_mask = (1 - attention_mask.unsqueeze(1)) * -10000.  # (N, 1, Lq, L)\n",
    "        query_layer = self.query(query_states)\n",
    "        key_layer = self.key(key_states)\n",
    "        value_layer = self.value(value_states)\n",
    "        # transpose\n",
    "        #query_layer = self.transpose_for_scores(mixed_query_layer)  # (N, nh, Lq, dh)\n",
    "        #key_layer = self.transpose_for_scores(mixed_key_layer)  # (N, nh, L, dh)\n",
    "        #value_layer = self.transpose_for_scores(mixed_value_layer)  # (N, nh, L, dh)\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        \n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # N x D   D x N\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1 - attention_mask.unsqueeze(1)) * -10000.  # (N, 1, Lq, L)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "        # attention_scores = attention_scores + attention_mask\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # compute output context\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.contiguous()\n",
    "        #new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        #context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        #print('attention_output:{}'.format(context_layer.shape))\n",
    "        return context_layer\n",
    "\n",
    "# class BertIntermediate(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super(BertIntermediate, self).__init__()\n",
    "#         self.dense = nn.Sequential(nn.Linear(config.hidden_size, config.intermediate_size), nn.ReLU(True))\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         return self.dense(hidden_states)\n",
    "\n",
    "\n",
    "# class BertOutput(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super(BertOutput, self).__init__()\n",
    "#         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "#         self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "#     def forward(self, hidden_states, input_tensor):\n",
    "#         hidden_states = self.dense(hidden_states)\n",
    "#         hidden_states = self.dropout(hidden_states)\n",
    "#         hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "#         return hidden_states\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "# class BertLayer(nn.Module):\n",
    "#     def __init__(self, config, use_self_attention=True):\n",
    "#         super(BertLayer, self).__init__()\n",
    "#         self.use_self_attention = use_self_attention\n",
    "#         if use_self_attention:\n",
    "#             self.attention = BertAttention(config)\n",
    "#         self.intermediate = BertIntermediate(config)\n",
    "#         self.output = BertOutput(config)\n",
    "\n",
    "#     def forward(self, hidden_states, attention_mask):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             hidden_states:  (N, L, D)\n",
    "#             attention_mask:  (N, L) with 1 indicate valid, 0 indicates invalid\n",
    "#         \"\"\"\n",
    "#         if self.use_self_attention:\n",
    "#             attention_output = self.attention(hidden_states, attention_mask)\n",
    "#         else:\n",
    "#             attention_output = hidden_states\n",
    "#         intermediate_output = self.intermediate(attention_output)\n",
    "#         layer_output = self.output(intermediate_output, attention_output)\n",
    "#         return layer_output\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: (N, L, D)\n",
    "            attention_mask: (N, L)\n",
    "        \"\"\"\n",
    "        self_output = self.self(input_tensor, input_tensor, input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tran_model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(tran_model,self).__init__()\n",
    "        self.config = config\n",
    "        self.attention_layer = BertAttention(config)\n",
    "        self.linear_layer = nn.Linear(config.hidden_size,2)\n",
    "    def reset_parameters(self):\n",
    "        def re_init(module):\n",
    "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "            elif isinstance(module, nn.Conv1d):\n",
    "                module.reset_parameters()\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        self.apply(re_init)\n",
    "    def forward(self,X_data,Y_data,attention_mask = None):\n",
    "        x = self.attention_layer(X_data,attention_mask)\n",
    "        x = self.linear_layer(x)\n",
    "        #print('x.shape:{}'.format(x.shape))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理一下数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "class Dataset_tran(data.Dataset):\n",
    "    def __init__(self,X_data,Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_data[index],self.Y_data[index]]\n",
    "    def __len__(self):\n",
    "        return self.X_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_Y = torch.tensor(np.array(train_X).astype('float32')),torch.tensor(np.array(train_Y).astype('float32'))\n",
    "val_X,val_Y = torch.tensor(np.array(val_X).astype('float32')),torch.tensor(np.array(val_Y).astype('float32'))\n",
    "test_X,test_Y = torch.tensor(np.array(test_X).astype('float32')),torch.tensor(np.array(test_Y).astype('float32'))\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_train(data):\n",
    "    #print('data[0]:{}'.format(data[0]))\n",
    "    #print('data[1]:{}'.format(data[1]))\n",
    "    return [data[0],data[1]]\n",
    "train_dataset,val_dataset,test_dataset = Dataset_tran(train_X,train_Y),Dataset_tran(val_X,val_Y),Dataset_tran(test_X,test_Y)\n",
    "train_loader = DataLoader(dataset = train_dataset,batch_size=8,shuffle = True)#,collate_fn= collate_train)\n",
    "val_loader = DataLoader(dataset = val_dataset,batch_size=8,shuffle = True)#,collate_fn= collate_train)\n",
    "test_loader = DataLoader(dataset = test_dataset,batch_size=8,shuffle = False)#,collate_fn= collate_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = tran_model(config)\n",
    "from tqdm import *\n",
    "optimizer = torch.optim.SGD(Model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "for batch_idx,batch in tqdm(enumerate(train_loader)):\n",
    "    #print('batch.shape:{}'.format(batch[0].shape))\n",
    "    pred_Y = Model(batch[0],batch[1])\n",
    "    loss = loss_fn(pred_Y,batch[1])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #print('batch_idx:{},loss:{}'.format(batch_idx,loss.item()))\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print('y_pred.shape:{}'.format(pred_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx,batch in tqdm(enumerate(test_loader)):\n",
    "    #print('batch.shape:{}'.format(batch[0].shape))\n",
    "    pred_Y = Model(batch[0],batch[1])\n",
    "    loss = loss_fn(pred_Y,batch[1])\n",
    "    #optimizer.zero_grad()\n",
    "    #loss.backward()\n",
    "    print('batch_idx:{},loss:{}'.format(batch_idx,loss.item()))\n",
    "    #optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('myDLE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71233b4bc7a057337e82d6affd910463f58de1fbc8542d0786f1e73be0d7f037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
